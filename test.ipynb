{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_graph() -> nx.Graph:\n",
    "    # Load the graph from the graphml file\n",
    "    g = nx.read_graphml(\"dataset/airportsAndCoordAndPop.graphml\")\n",
    "    return g\n",
    "\n",
    "def filter_country(graph: nx.Graph, nb_airports: int) -> nx.Graph:\n",
    "    g = graph.copy()\n",
    "    countries = set([g.nodes[node][\"country\"] for node in g.nodes])\n",
    "    countries = {country: sum([1 for node in g.nodes if g.nodes[node][\"country\"] == country]) for country in countries}\n",
    "    g = g.subgraph([node for node in g.nodes if countries[g.nodes[node][\"country\"]] >= nb_airports])    \n",
    "    return g\n",
    "\n",
    "def convert_fc(graph: nx.Graph) -> nx.Graph:\n",
    "    g = graph.copy()\n",
    "    g.remove_edges_from(g.edges)\n",
    "    edges = list(set([tuple(sorted([n1, n2])) for n1 in g.nodes for n2 in g.nodes if n1 != n2]))\n",
    "    g.add_edges_from(edges)\n",
    "    return g\n",
    "\n",
    "\n",
    "def connect_country(graph: nx.Graph) -> nx.Graph:\n",
    "    g = graph.copy()\n",
    "    g.remove_edges_from(g.edges)\n",
    "    for node in g.nodes:\n",
    "        country = g.nodes[node][\"country\"]\n",
    "        for node2 in g.nodes:\n",
    "            if g.nodes[node2][\"country\"] == country and node != node2:\n",
    "                g.add_edge(node, node2)\n",
    "    return g\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    import math\n",
    "\n",
    "    R = 6371  # radius of the Earth in km\n",
    "    lat1 = math.radians(lat1)\n",
    "    lon1 = math.radians(lon1)\n",
    "    lat2 = math.radians(lat2)\n",
    "    lon2 = math.radians(lon2)\n",
    "\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    a = math.sin(dlat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "    return R * c\n",
    "\n",
    "def add_distance(g: nx.Graph) -> nx.Graph:\n",
    "    new_graph = g.copy()\n",
    "    for edge in new_graph.edges:\n",
    "        n1, n2 = edge\n",
    "        new_graph.edges[edge][\"distance\"] = haversine(\n",
    "            new_graph.nodes[n1][\"lat\"],\n",
    "            new_graph.nodes[n1][\"lon\"],\n",
    "            new_graph.nodes[n2][\"lat\"],\n",
    "            new_graph.nodes[n2][\"lon\"],\n",
    "        )\n",
    "        \n",
    "    return new_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load: \t\t\t\tGraph with 3363 nodes and 13547 edges (0.198418s)\n",
      "Filter(100): \t\t\tGraph with 1205 nodes and 3884 edges (0.110703s)\n",
      "Convert to fully-connected: \tGraph with 1205 nodes and 725410 edges (1.320693s)\n",
      "Add distance on edges: \t\tGraph with 1205 nodes and 725410 edges (2.303417s)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "st = time.time()\n",
    "g = load_graph()\n",
    "print(f\"Load: \\t\\t\\t\\t{g} ({time.time() - st:3f}s)\")\n",
    "\n",
    "st = time.time()\n",
    "nb = 100\n",
    "g = filter_country(g, nb_airports=nb)\n",
    "print(f\"Filter({nb}): \\t\\t\\t{g} ({time.time() - st:3f}s)\")\n",
    "\n",
    "st = time.time()\n",
    "g = convert_fc(g)\n",
    "print(f\"Convert to fully-connected: \\t{g} ({time.time() - st:3f}s)\")\n",
    "\n",
    "st = time.time()\n",
    "g = add_distance(g)\n",
    "print(f\"Add distance on edges: \\t\\t{g} ({time.time() - st:3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_networkx\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "def encode_country(data: list[str]) -> tuple[torch.Tensor, np.ndarray]:\n",
    "    encoder = LabelEncoder()\n",
    "    data_encoded = encoder.fit_transform(data)\n",
    "    classes = encoder.classes_\n",
    "    return torch.tensor(data_encoded, dtype=torch.long), classes\n",
    "\n",
    "def normalize(tensor: torch.Tensor):\n",
    "    scaler = StandardScaler()\n",
    "    tensor_transformed = torch.tensor(scaler.fit_transform(tensor), dtype=torch.float)\n",
    "    return tensor_transformed\n",
    "\n",
    "\n",
    "def prepart_data(\n",
    "    graph: nx.Graph, \n",
    "    node_attr: list[str],\n",
    "    with_distance: bool = True,\n",
    "    train_ratio: float = 0.8, \n",
    "    test_ratio: float = 0.1,\n",
    "    val_ratio: float = 0.1\n",
    "    ):\n",
    "    \n",
    "    # Création de mask pour les données d'entraînement, de validation et de test\n",
    "    if train_ratio + test_ratio + val_ratio!= 1:\n",
    "        raise ValueError(\"train_ratio + test_ratio must be equal to 1\")\n",
    "    \n",
    "    # Convert\n",
    "    data: Data = from_networkx(graph, group_node_attrs=node_attr)\n",
    "    if with_distance:\n",
    "        dist = data.distance.clone().detach()\n",
    "        data.edge_weight = dist.reshape(-1, 1)\n",
    "        # Normalize edge_weight\n",
    "        data.edge_weight = normalize(data.edge_weight)\n",
    "        data.edge_weight = 1 - (data.edge_weight - data.edge_weight.min()) / (data.edge_weight.max() - data.edge_weight.min())\n",
    "    else:\n",
    "        data.edge_weight = torch.ones(data.num_edges, 1)\n",
    "    \n",
    "    # Encode Y (country label) \n",
    "    data.y, data.y_classes = encode_country(data.country) \n",
    "    \n",
    "    # Normalize X\n",
    "    data.x = normalize(data.x)\n",
    "    \n",
    "    \n",
    "    # Mask train/test/val\n",
    "    data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    data.test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    data.val_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    data.train_mask[:int(data.num_nodes * train_ratio)] = True\n",
    "    data.test_mask[int(data.num_nodes * train_ratio):int(data.num_nodes * train_ratio)+int(data.num_nodes * test_ratio)] = True\n",
    "    data.val_mask[int(data.num_nodes * train_ratio)+int(data.num_nodes * test_ratio):] = True\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "g = load_graph()\n",
    "g = filter_country(g, 10)\n",
    "g = add_distance(g)\n",
    "print(g)\n",
    "data = prepart_data(g, [\"lon\", \"lat\"])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as gnn\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, dim_in, dim_h, dim_out):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = gnn.GCNConv(dim_in, dim_h)\n",
    "        self.conv2 = gnn.GCNConv(dim_h, dim_h)\n",
    "        self.conv3 = gnn.GCNConv(dim_h, dim_out)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_weight))\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_weight))\n",
    "        x = self.conv3(x, edge_index, edge_weight)\n",
    "        return  F.log_softmax(x, dim=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "def evaluate(model: GCN, data: Data, mask=None, loss_fct=None) -> tuple[float, float]:\n",
    "    if mask is None:\n",
    "        mask = np.ones(data.y.shape[0], dtype=bool)\n",
    "    if loss_fct is None:\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index, data.edge_weight)\n",
    "        pred = out[mask].argmax(dim=1)\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / data.y[mask].shape[0]\n",
    "        \n",
    "        loss = loss_fct(out[mask], data.y[mask])\n",
    "        \n",
    "        return acc, loss.item()\n",
    "    \n",
    "    \n",
    "def predict(gcn, x, ei, ew):\n",
    "    with torch.no_grad():\n",
    "            out = gcn(x, ei, ew)\n",
    "            return out.argmax(dim=1)\n",
    "    \n",
    "\n",
    "def train(model: GCN, data: Data, epochs=100, lr=0.01, writer=None):\n",
    "    if writer is None:\n",
    "        writer = SummaryWriter(comment=f\"_gcn_lr[{lr}]\")\n",
    "        \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_weights = model.state_dict()\n",
    "    best_loss = 1e9\n",
    "    count_no_improve = 0\n",
    "    \n",
    "    for ep in range(epochs):\n",
    "        # TRAIN\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        out = model(data.x, data.edge_index, data.edge_weight)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fct(out[data.train_mask], data.y[data.train_mask])\n",
    "        # Compute accuracy\n",
    "        pred = out[data.train_mask].argmax(dim=1)\n",
    "        acc = pred.eq(data.y[data.train_mask]).sum().item() / data.y[data.train_mask].shape[0]\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        writer.add_scalar(\"Train (loss)\", loss.item(), ep)\n",
    "        writer.add_scalar(\"Train (acc)\", acc, ep)\n",
    "        \n",
    "        # EVALUATE\n",
    "        val_acc, val_loss = evaluate(model, data, data.val_mask)\n",
    "        \n",
    "        writer.add_scalar(\"Val (acc)\", val_acc, ep)\n",
    "        writer.add_scalar(\"Val (loss)\", val_loss, ep)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss <= best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_weights = model.state_dict()\n",
    "            count_no_improve = 0\n",
    "        else:\n",
    "            count_no_improve += 1\n",
    "            if count_no_improve > 100:\n",
    "                print(f\"Early stopping at epoch {ep}\")\n",
    "                break\n",
    "        \n",
    "        print(f\"\\rEpoch {ep+1}/{epochs} - Loss={val_loss} - Acc={val_acc}\" + \" \" * 20, end=\"\")\n",
    "    \n",
    "    model.load_state_dict(best_weights)\n",
    "    val_acc = evaluate(model, data, data.test_mask)\n",
    "    print(f\"\\nTest accuracy: {val_acc}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the graph...\n",
      "Data(edge_index=[2, 534218], population=[1205], country=[1205], city_name=[1205], x=[1205, 2], edge_weight=[534218, 1], y=[1205], y_classes=[4], train_mask=[1205], test_mask=[1205], val_mask=[1205])\n",
      "\n",
      "Creating the model...\n",
      "GCN(\n",
      "  (conv1): GCNConv(2, 16)\n",
      "  (conv2): GCNConv(16, 16)\n",
      "  (conv3): GCNConv(16, 4)\n",
      ")\n",
      "Training the model...\n",
      "Epoch 2986/10000 - Loss=0.001323458505794406 - Acc=1.0                                 "
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "lr = 0.001\n",
    "filter_nb = 100\n",
    "distance = False\n",
    "full_connected = False\n",
    "country_connected = True\n",
    "node_attr = [\"lon\", \"lat\"]\n",
    "\n",
    "# Tensorboard\n",
    "writer = SummaryWriter(comment=f\"_gcn_lr[{lr}]_filter[{filter_nb}]_cc[{country_connected}]_distance[{distance}]_node_attr[{\",\".join(node_attr)}]\")\n",
    "\n",
    "# Load the graph\n",
    "print(\"Loading the graph...\")\n",
    "g = load_graph()\n",
    "g = filter_country(g, filter_nb)\n",
    "if full_connected:\n",
    "    g = convert_fc(g)\n",
    "if country_connected:\n",
    "    g = connect_country(g)\n",
    "if distance:\n",
    "    g = add_distance(g)   \n",
    "# Prepare the data\n",
    "data = prepart_data(g, node_attr, with_distance=distance)\n",
    "print(str(data) + \"\\n\")\n",
    "\n",
    "# Create the model\n",
    "print(\"Creating the model...\")\n",
    "model = GCN(data.num_node_features, 16, data.y_classes.shape[0])\n",
    "print(model)\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "model = train(model, data, epochs=10000, lr=lr, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
